{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "import torch._utils\n",
    "from transformers import pipeline\n",
    "\n",
    "#for not seing a warning message\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def get_text_embedding(text, model_name='bert-base-uncased'):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize input text and convert to PyTorch tensors\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Get output from pre-trained model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract last layer of output (CLS token) as the text embedding\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_similar_paragraphs(query_embedding, paragraph_embeddings, paragraphs, k=5):\n",
    "\n",
    "    # Compute the cosine similarities between the query embedding and each paragraph embedding\n",
    "    similarities = [cosine_similarity(query_embedding.reshape(1, -1), embedding.reshape(1, -1)) for embedding in paragraph_embeddings]\n",
    "\n",
    "    # Get the indices of the top k paragraphs based on their similarity scores\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:k]\n",
    "\n",
    "    # Return the top k paragraphs and their similarity scores as a list dictionaries\n",
    "    return [{\"content\": paragraphs[i], \"score\" :similarities[i]} for i in top_indices]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "query=\"Did the Greeks and the Romans have universities?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[144], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m paragraphs \u001B[38;5;241m=\u001B[39m input_text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#Get embeddings of paragraphs and query\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m [get_text_embedding(paragraph) \u001B[38;5;28;01mfor\u001B[39;00m paragraph \u001B[38;5;129;01min\u001B[39;00m paragraphs]\n\u001B[0;32m     13\u001B[0m query_embedding \u001B[38;5;241m=\u001B[39m get_text_embedding(query)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m#Get top k similar paragraphs and cosine distance score\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[144], line 12\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      9\u001B[0m paragraphs \u001B[38;5;241m=\u001B[39m input_text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#Get embeddings of paragraphs and query\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m [\u001B[43mget_text_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparagraph\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m paragraph \u001B[38;5;129;01min\u001B[39;00m paragraphs]\n\u001B[0;32m     13\u001B[0m query_embedding \u001B[38;5;241m=\u001B[39m get_text_embedding(query)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m#Get top k similar paragraphs and cosine distance score\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[141], line 13\u001B[0m, in \u001B[0;36mget_text_embedding\u001B[1;34m(text, model_name)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_text_embedding\u001B[39m(text, model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# Load pre-trained model and tokenizer\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[1;32m---> 13\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# Tokenize input text and convert to PyTorch tensors\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tokenizer(text, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    462\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    464\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    465\u001B[0m     )\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    469\u001B[0m )\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2276\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2273\u001B[0m     init_contexts\u001B[38;5;241m.\u001B[39mappend(init_empty_weights())\n\u001B[0;32m   2275\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[1;32m-> 2276\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(config, \u001B[38;5;241m*\u001B[39mmodel_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[0;32m   2278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_in_8bit:\n\u001B[0;32m   2279\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:890\u001B[0m, in \u001B[0;36mBertModel.__init__\u001B[1;34m(self, config, add_pooling_layer)\u001B[0m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(config)\n\u001B[0;32m    888\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m--> 890\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings \u001B[38;5;241m=\u001B[39m \u001B[43mBertEmbeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m BertEncoder(config)\n\u001B[0;32m    893\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;241m=\u001B[39m BertPooler(config) \u001B[38;5;28;01mif\u001B[39;00m add_pooling_layer \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:186\u001B[0m, in \u001B[0;36mBertEmbeddings.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config):\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m--> 186\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mword_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embeddings \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(config\u001B[38;5;241m.\u001B[39mmax_position_embeddings, config\u001B[38;5;241m.\u001B[39mhidden_size)\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_type_embeddings \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(config\u001B[38;5;241m.\u001B[39mtype_vocab_size, config\u001B[38;5;241m.\u001B[39mhidden_size)\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:142\u001B[0m, in \u001B[0;36mEmbedding.__init__\u001B[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m Parameter(torch\u001B[38;5;241m.\u001B[39mempty((num_embeddings, embedding_dim), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfactory_kwargs))\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_weight\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m [num_embeddings, embedding_dim], \\\n\u001B[0;32m    145\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:151\u001B[0m, in \u001B[0;36mEmbedding.reset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset_parameters\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 151\u001B[0m     \u001B[43minit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fill_padding_idx_with_zero()\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\torch\\nn\\init.py:155\u001B[0m, in \u001B[0;36mnormal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39moverrides\u001B[38;5;241m.\u001B[39mhas_torch_function_variadic(tensor):\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39moverrides\u001B[38;5;241m.\u001B[39mhandle_torch_function(normal_, (tensor,), tensor\u001B[38;5;241m=\u001B[39mtensor, mean\u001B[38;5;241m=\u001B[39mmean, std\u001B[38;5;241m=\u001B[39mstd)\n\u001B[1;32m--> 155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_no_grad_normal_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\DTU - KID\\4. Semester\\Bachelor projekt AI\\KODE\\ChatGPT_Tutor_Project\\venv\\lib\\site-packages\\torch\\nn\\init.py:19\u001B[0m, in \u001B[0;36m_no_grad_normal_\u001B[1;34m(tensor, mean, std)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_no_grad_normal_\u001B[39m(tensor, mean, std):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 19\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#Open example text file\n",
    "path=\"Documents/\"\n",
    "file_name=\"rise_of_universities.txt\"\n",
    "\n",
    "with open(path+file_name, \"r\",encoding='utf-8') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "#split into paragraphs\n",
    "paragraphs = input_text.split(\"\\n\\n\")\n",
    "\n",
    "#Get embeddings of paragraphs and query\n",
    "embeddings = [get_text_embedding(paragraph) for paragraph in paragraphs]\n",
    "query_embedding = get_text_embedding(query)\n",
    "\n",
    "#Get top k similar paragraphs and cosine distance score\n",
    "similar_paragraphs = get_similar_paragraphs(query_embedding, embeddings,paragraphs, k=5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "path = 'Documents/02450_w_form.txt'\n",
    "with open(path, \"r\", encoding='utf-8') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "#split into paragraphs\n",
    "paragraphs = input_text.split(\"\\n\\n\")\n",
    "paragraphs = [p for p in paragraphs if len(p) > 50]\n",
    "#Remove \\n and \\t and -  from paragraphs:\n",
    "paragraphs = [p.replace(\"\\n\", \" \") for p in paragraphs]\n",
    "paragraphs = [p.replace(\"\\t\", \" \") for p in paragraphs]\n",
    "paragraphs = [p.replace(\"- \", \" \") for p in paragraphs]\n",
    "\n",
    "# keep adding paragraphs together so they each have a minimum length of 700 characters\n",
    "def add_paragraphs(paragraphs):\n",
    "    i = 0\n",
    "    while i < len(paragraphs) - 1:\n",
    "        if len(paragraphs[i]) < 700:\n",
    "            paragraphs[i] = paragraphs[i] + paragraphs[i + 1]\n",
    "            paragraphs.pop(i + 1)\n",
    "        else:\n",
    "            i += 1\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "paragraphs = add_paragraphs(paragraphs)\n",
    "\n",
    "#Get embeddings of paragraphs and query\n",
    "embeddings = [get_text_embedding(paragraph) for paragraph in paragraphs]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "\n",
    "# with open(\"emb_02450\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(embeddings, fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "with open(\"emb_02450\", \"rb\") as fp:\n",
    "    embeddings_02450 = pickle.load(fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "query =\"How does the Expectation maximization algorithm works?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "query_embedding = get_text_embedding(query)\n",
    "\n",
    "#Get top k similar paragraphs and cosine distance score\n",
    "similar_paragraphs = get_similar_paragraphs(query_embedding, embeddings_02450,paragraphs, k=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where  λ > 0  is  the  regularization  term.  This  difficulty  increases  with  poor   initialization  and  it  is therefore  recommended  to  initialize  the  EM  algorithm  to  the   output  of  the  K-means  clustering algorithm. Third, the EM algorithm in its present form  requires parameters;  for  high-dimensional  datasets  the  number  K(M  + 1)M/2  can  be  brought  down  by  considering  a  diagonal  covariance  matrix  to  KM .  There  is  however  also  goods  news  with   regards to  the  EM  algorithm  for  GMMs.  Asides  accomplishing  the  primary  objective,  a   general  density estimator which can be fitted efficiently, an advantage of the EM algorithm over   K-means is that one can select K  using cross-validation.\n"
     ]
    }
   ],
   "source": [
    "#The top 5 paragraphs\n",
    "top_answers= similar_paragraphs[0]['content']\n",
    "print(top_answers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "oracle = pipeline(model=\"deepset/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n",
    "Bert_topK = oracle(query, top_answers, top_k=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.017856039106845856,\n  'start': 728,\n  'end': 744,\n  'answer': 'cross-validation'},\n {'score': 0.010887210257351398,\n  'start': 704,\n  'end': 744,\n  'answer': 'one can select K  using cross-validation'},\n {'score': 0.010439837351441383,\n  'start': 722,\n  'end': 744,\n  'answer': 'using cross-validation'},\n {'score': 0.0030739835929125547,\n  'start': 712,\n  'end': 744,\n  'answer': 'select K  using cross-validation'},\n {'score': 0.002891442272812128,\n  'start': 708,\n  'end': 744,\n  'answer': 'can select K  using cross-validation'}]"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_topK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "prompt=f\"You are a friendly and helpful chatbot. your job is to give a short and relevant answer to a given question, based on a specific background context. The context is: '{top_answers}'. Please give a relevant answer to the following question. Question: {query}. Answer: {Bert_topK[0]['answer']}\"\n",
    "\n",
    "# prompt=f\"You are a friendly and helpful chatbot. your job is to give a short and relevant answer to a given question. Please give a relevant answer to the following question. Question: {query}. Answer:\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': 'You are a friendly and helpful chatbot. your job is to give a short and relevant answer to a given question, based on a specific background context. The context is: \\'where  λ > 0  is  the  regularization  term.  This  difficulty  increases  with  poor   initialization  and  it  is therefore  recommended  to  initialize  the  EM  algorithm  to  the   output  of  the  K-means  clustering algorithm. Third, the EM algorithm in its present form  requires parameters;  for  high-dimensional  datasets  the  number  K(M  + 1)M/2  can  be  brought  down  by  considering  a  diagonal  covariance  matrix  to  KM .  There  is  however  also  goods  news  with   regards to  the  EM  algorithm  for  GMMs.  Asides  accomplishing  the  primary  objective,  a   general  density estimator which can be fitted efficiently, an advantage of the EM algorithm over   K-means is that one can select K  using cross-validation.\\'. Please give a relevant answer to the following question. Question: How does the Expectation maximization algorithm works?. Answer: cross-validation is a type of algorithm for computing probability distributions over variable x 1, where x 1 is the data vector x 2. Using the cross-validation algorithm to determine the optimal function for parameter x 1, the parameter x 2 that maximizes the probability given x 2 is called the \"optimal function\" and the optimum must include the variance of the data vector x 2. To see the difference between the parameters and the resulting probabilities, draw a number of dots in the shape of the cross-validation (sketch) and compare the actual probability distribution for parameter x 2 ; the cross'}]"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feed the relevant sentences and query to a GPT model to generate a response\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2-large\")\n",
    "generator(prompt, max_length=400, do_sample=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
