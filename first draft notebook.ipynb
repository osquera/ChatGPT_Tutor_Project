{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "#for not seing a warning message\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "def get_text_embedding(text, model_name='bert-base-uncased'):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize input text and convert to PyTorch tensors\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Get output from pre-trained model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract last layer of output (CLS token) as the text embedding\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_similar_paragraphs(query_embedding, paragraph_embeddings, paragraphs, k=5):\n",
    "\n",
    "    # Compute the cosine similarities between the query embedding and each paragraph embedding\n",
    "    similarities = [cosine_similarity(query_embedding.reshape(1, -1), embedding.reshape(1, -1)) for embedding in paragraph_embeddings]\n",
    "\n",
    "    # Get the indices of the top k paragraphs based on their similarity scores\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:k]\n",
    "\n",
    "    # Return the top k paragraphs and their similarity scores as a list dictionaries\n",
    "    return [{\"content\": paragraphs[i], \"score\" :similarities[i]} for i in top_indices]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Open example text file\n",
    "path=\"C:/Users/farim/PycharmProjects/ChatGPT_Tutor_Project/Documents/\"\n",
    "file_name=\"text_example.txt\"\n",
    "\n",
    "with open(path+file_name, \"r\",encoding='utf-8') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "#split into paragraphs\n",
    "paragraphs = input_text.split(\"\\n\\n\")\n",
    "\n",
    "#Get embeddings of paragraphs and query\n",
    "embeddings = [get_text_embedding(paragraph) for paragraph in paragraphs]\n",
    "query_embedding = get_text_embedding(\"Did the Greeks and the Romans have universities?\")\n",
    "\n",
    "#Get top k similar paragraphs and cosine distance score\n",
    "similar_paragraphs = get_similar_paragraphs(query_embedding, embeddings,paragraphs, k=5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'THE EARLIEST UNIVERSITIES\\nUniversities, like cathedrals and parliaments, are a product of the Middle Ages. The Greeks and the Romans, strange as it may seem, had no universities in the sense in which the word has been used for the past seven or eight centuries. They had higher education, but the terms are not synonymous. Much of their instruction in law, rhetoric, and philosophy it would be hard to surpass, but it was not organized into the form of permanent institutions of learning. A great teacher like Socrates gave no diplomas; if a modern student sat at his feet for three months, he would demand a certificate,[4] something tangible and external to show for itâ€”an excellent theme, by the way, for a Socratic dialogue. Only in the twelfth and thirteenth centuries do there emerge in the world those features of organized education with which we are most familiar, all that machinery of instruction represented by faculties and colleges and courses of study, examinations and commencements and academic degrees. In all these matters we are the heirs and successors, not of Athens and Alexandria, but of Paris and Bologna.', 'score': array([[0.6883316]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "#print most similar paragraphs and score\n",
    "print(similar_paragraphs[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
